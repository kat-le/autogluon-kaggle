{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kat-le/autogluon-kaggle/blob/main/ieee_fraud_autogluon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa7eb3d",
      "metadata": {
        "id": "0fa7eb3d"
      },
      "source": [
        "# How to use AutoGluon for Kaggle competitions\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb)\n",
        "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "This tutorial will teach you how to use AutoGluon to become a serious Kaggle competitor without writing lots of code.\n",
        "We first outline the general steps to use AutoGluon in Kaggle contests. Here, we assume the competition involves tabular data which are stored in one (or more) CSV files.\n",
        "\n",
        "1) Run Bash command: pip install kaggle\n",
        "\n",
        "2) Navigate to: https://www.kaggle.com/account and create an account (if necessary).\n",
        "Then , click on \"Create New API Token\" and move downloaded file to this location on your machine: `~/.kaggle/kaggle.json`. For troubleshooting, see [Kaggle API instructions](https://www.kaggle.com/docs/api).\n",
        "\n",
        "3) To download data programmatically: Execute this Bash command in your terminal:\n",
        "\n",
        "`kaggle competitions download -c [COMPETITION]`\n",
        "\n",
        "Here, [COMPETITION] should be replaced by the name of the competition you wish to enter.\n",
        "Alternatively, you can download data manually: Just navigate to website of the Kaggle competition you wish to enter, click \"Download All\", and accept the competition's terms.\n",
        "\n",
        "4) If the competition's training data is comprised of multiple CSV files, use [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) to properly merge/join them into a single data table where rows = training examples, columns = features.\n",
        "\n",
        "5) Run autogluon `fit()` on the resulting data table.\n",
        "\n",
        "6) Load the test dataset from competition (again making the necessary merges/joins to ensure it is in the exact same format as the training data table), and then call autogluon `predict()`.  Subsequently use [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to load the competition's `sample_submission.csv` file into a DataFrame, put the AutoGluon predictions in the right column of this DataFrame, and finally save it as a CSV file via [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html). If the competition does not offer a sample submission file, you will need to create the submission file yourself by appropriately reformatting AutoGluon's test predictions.\n",
        "\n",
        "7) Submit your predictions via Bash command:\n",
        "\n",
        "`kaggle competitions submit -c [COMPETITION] -f [FILE] -m [\"MESSAGE\"]`\n",
        "\n",
        "Here, [COMPETITION] again is the competition's name, [FILE] is the name of the CSV file you created with your predictions, and [\"MESSAGE\"] is a string message you want to record with this submitted entry. Alternatively, you can  manually upload your file of predictions on the competition website.\n",
        "\n",
        "8) Finally, navigate to competition leaderboard website to see how well your submission performed!\n",
        "It may take time for your submission to appear.\n",
        "\n",
        "\n",
        "\n",
        "Below, we demonstrate how to do steps (4)-(6) in Python for a specific Kaggle competition: [ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection/).\n",
        "This means you'll need to run the above steps with `[COMPETITION]` replaced by `ieee-fraud-detection` in each command.  Here, we assume you've already completed steps (1)-(3) and the data CSV files are available on your computer. To begin step (4), we first load the competition's training data into Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daajxZ6j3PkQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daajxZ6j3PkQ",
        "outputId": "b58e2ddd-978c-4c60-9b2b-343ba56b5ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.12\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Tue Oct 14 00:04:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   36C    P0             52W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Working dir: /content/IEEEfraud/\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U pip\n",
        "!pip -q install -U autogluon kaggle\n",
        "\n",
        "import autogluon, sys, platform, os, pandas as pd, numpy as np\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "!nvidia-smi || true\n",
        "\n",
        "DIR = \"/content/IEEEfraud/\"\n",
        "os.makedirs(DIR, exist_ok=True)\n",
        "print(\"Working dir:\", DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "obq2H8ag4jHo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obq2H8ag4jHo",
        "outputId": "52d980e0-606b-43af-8332-adc7421d1f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GZfDnIfJ3U3n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "GZfDnIfJ3U3n",
        "outputId": "c6bb40a1-c2e3-4280-d7b2-449c5069ea1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76289f60-38b1-4521-9a65-4382dea2fc5c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76289f60-38b1-4521-9a65-4382dea2fc5c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "total 4\n",
            "-rw------- 1 root root 65 Oct 14 00:04 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls -l ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3DrdZjvT3ZwD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DrdZjvT3ZwD",
        "outputId": "1f3f8570-6271-4f75-8d74-9f8157678a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading ieee-fraud-detection.zip to /content/IEEEfraud\n",
            "  0% 0.00/118M [00:00<?, ?B/s]\n",
            "100% 118M/118M [00:00<00:00, 1.69GB/s]\n",
            "total 1.4G\n",
            "-rw-r--r-- 1 root root 119M Dec 11  2019 ieee-fraud-detection.zip\n",
            "-rw-r--r-- 1 root root 5.8M Dec 11  2019 sample_submission.csv\n",
            "-rw-r--r-- 1 root root  25M Dec 11  2019 test_identity.csv\n",
            "-rw-r--r-- 1 root root 585M Dec 11  2019 test_transaction.csv\n",
            "-rw-r--r-- 1 root root  26M Dec 11  2019 train_identity.csv\n",
            "-rw-r--r-- 1 root root 652M Dec 11  2019 train_transaction.csv\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!kaggle competitions download -c ieee-fraud-detection -p \"{DIR}\"\n",
        "!unzip -o -q \"{DIR}/ieee-fraud-detection.zip\" -d \"{DIR}\"\n",
        "!ls -lh \"{DIR}\" | head -n 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nqMRzvQU1Ot8",
      "metadata": {
        "id": "nqMRzvQU1Ot8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "directory = '/content/IEEEfraud/'\n",
        "label = 'isFraud'\n",
        "eval_metric = 'roc_auc'\n",
        "save_path = directory + 'AutoGluonModels/'\n",
        "\n",
        "train_identity = pd.read_csv(directory+'train_identity.csv')\n",
        "train_transaction = pd.read_csv(directory+'train_transaction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b046735",
      "metadata": {
        "id": "1b046735"
      },
      "source": [
        "Since the training data for this competition is comprised of multiple CSV files, we just first join them into a single large table (with rows = examples, columns = features) before applying AutoGluon:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WaK1qM757qEs",
      "metadata": {
        "id": "WaK1qM757qEs"
      },
      "outputs": [],
      "source": [
        "train_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de8c287",
      "metadata": {
        "id": "7de8c287"
      },
      "source": [
        "Note that a left-join on the `TransactionID` key happened to be most appropriate for this Kaggle competition, but for others involving multiple training data files, you will likely need to use a different join strategy (always consider this very carefully). Now that all our training data resides within a single table, we can apply AutoGluon. Below, we specify the `presets` argument to maximize AutoGluon's predictive accuracy which usually requires that you run `fit()` with longer time limits (3600s below should likely be increased in your run):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaad348b",
      "metadata": {
        "id": "aaad348b"
      },
      "source": [
        "```\n",
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
        "    train_data, presets='best_quality', time_limit=3600\n",
        ")\n",
        "\n",
        "results = predictor.fit_summary()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CcvaY97B7v_S",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcvaY97B7v_S",
        "outputId": "2c982a24-1f0d-4bc2-d28c-7e4865dd7009"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verbosity: 3 (Detailed Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          12\n",
            "GPU Count:          1\n",
            "Memory Avail:       159.03 GB / 167.05 GB (95.2%)\n",
            "Disk Space Avail:   188.69 GB / 235.68 GB (80.1%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "============ fit kwarg info ============\n",
            "User Specified kwargs:\n",
            "{'auto_stack': False}\n",
            "Full kwargs:\n",
            "{'_experimental_dynamic_hyperparameters': False,\n",
            " '_feature_generator_kwargs': None,\n",
            " '_save_bag_folds': None,\n",
            " 'ag_args': None,\n",
            " 'ag_args_ensemble': None,\n",
            " 'ag_args_fit': None,\n",
            " 'auto_stack': False,\n",
            " 'calibrate': 'auto',\n",
            " 'delay_bag_sets': False,\n",
            " 'ds_args': {'clean_up_fits': True,\n",
            "             'detection_time_frac': 0.25,\n",
            "             'enable_callbacks': False,\n",
            "             'enable_ray_logging': True,\n",
            "             'holdout_data': None,\n",
            "             'holdout_frac': 0.1111111111111111,\n",
            "             'memory_safe_fits': True,\n",
            "             'n_folds': 2,\n",
            "             'n_repeats': 1,\n",
            "             'validation_procedure': 'holdout'},\n",
            " 'excluded_model_types': None,\n",
            " 'feature_generator': 'auto',\n",
            " 'feature_prune_kwargs': None,\n",
            " 'holdout_frac': None,\n",
            " 'hyperparameter_tune_kwargs': None,\n",
            " 'included_model_types': None,\n",
            " 'keep_only_best': False,\n",
            " 'learning_curves': False,\n",
            " 'name_suffix': None,\n",
            " 'num_bag_folds': None,\n",
            " 'num_bag_sets': None,\n",
            " 'num_stack_levels': None,\n",
            " 'pseudo_data': None,\n",
            " 'raise_on_model_failure': False,\n",
            " 'raise_on_no_models_fitted': True,\n",
            " 'refit_full': False,\n",
            " 'save_bag_folds': None,\n",
            " 'save_space': False,\n",
            " 'set_best_to_refit_full': False,\n",
            " 'test_data': None,\n",
            " 'unlabeled_data': None,\n",
            " 'use_bag_holdout': False,\n",
            " 'verbosity': 3}\n",
            "========================================\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Saving /content/IEEEfraud/AutoGluonModels/learner.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/predictor.pkl\n",
            "Beginning AutoGluon training ... Time limit = 900s\n",
            "AutoGluon will save models to \"/content/IEEEfraud/AutoGluonModels\"\n",
            "Train Data Rows:    590540\n",
            "Train Data Columns: 433\n",
            "Label Column:       isFraud\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [np.int64(0), np.int64(1)]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    163185.15 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2531.61 MB (1.6% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
            "\t\t\t\t('float64', 'float') : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int64', 'int')     :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t\t('object', 'object') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t4.1s = Fit runtime\n",
            "\t\t\t433 features in original data used to generate 433 features in processed data.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])  : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t2.2s = Fit runtime\n",
            "\t\t\t433 features in original data used to generate 433 features in processed data.\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', []) : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])   :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', []) : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])   :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t0.8s = Fit runtime\n",
            "\t\t\t402 features in original data used to generate 402 features in processed data.\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\t0.1s = Fit runtime\n",
            "\t\t\t\t31 features in original data used to generate 31 features in processed data.\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('object', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', []) : 31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t1.4s = Fit runtime\n",
            "\t\t\t31 features in original data used to generate 31 features in processed data.\n",
            "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
            "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
            "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
            "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
            "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\t('float', [])    : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\t('float', [])    : 399 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t3.7s = Fit runtime\n",
            "\t\t\t433 features in original data used to generate 433 features in processed data.\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\t\t4 duplicate columns removed: ['V28', 'V154', 'V155', 'V156']\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\t('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t\t\t('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t\t4.3s = Fit runtime\n",
            "\t\t\t429 features in original data used to generate 429 features in processed data.\n",
            "\tUnused Original Features (Count: 4): ['V28', 'V154', 'V155', 'V156']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 4 | ['V28', 'V154', 'V155', 'V156']\n",
            "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
            "\t\t('float64', 'float') : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int64', 'int')     :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('object', 'object') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
            "\t\t('category', 'category') :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t('float64', 'float')     : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int64', 'int')         :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t25.1s = Fit runtime\n",
            "\t429 features in original data used to generate 429 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1811.77 MB (1.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 29.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Saving /content/IEEEfraud/AutoGluonModels/learner.pkl\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 584634, Val Rows: 5906\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/data/X.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/data/y.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/data/X_val.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/data/y_val.pkl\n",
            "Model configs that will be trained (in order):\n",
            "\tLightGBMXT: \t{'extra_trees': True, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
            "\tLightGBM: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
            "\tRandomForestGini: \t{'criterion': 'gini', 'ag_args': {'problem_types': ['binary', 'multiclass'], 'name_suffix': 'Gini', 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tRandomForestEntr: \t{'criterion': 'entropy', 'ag_args': {'problem_types': ['binary', 'multiclass'], 'name_suffix': 'Entr', 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tCatBoost: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
            "\tExtraTreesGini: \t{'criterion': 'gini', 'ag_args': {'problem_types': ['binary', 'multiclass'], 'name_suffix': 'Gini', 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tExtraTreesEntr: \t{'criterion': 'entropy', 'ag_args': {'problem_types': ['binary', 'multiclass'], 'name_suffix': 'Entr', 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tNeuralNetFastAI: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
            "\tXGBoost: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
            "\tNeuralNetTorch: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
            "\tLightGBMLarge: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 870.55s of the 870.54s of remaining time.\n",
            "\tFitting LightGBMXT with 'num_gpus': 0, 'num_cpus': 6\n",
            "\tFitting with cpus=6, gpus=0, mem=9.7/155.4 GB\n",
            "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50]\tvalid_set's binary_logloss: 0.100743\n",
            "[100]\tvalid_set's binary_logloss: 0.0924349\n",
            "[150]\tvalid_set's binary_logloss: 0.0884887\n",
            "[200]\tvalid_set's binary_logloss: 0.0856469\n",
            "[250]\tvalid_set's binary_logloss: 0.0834745\n",
            "[300]\tvalid_set's binary_logloss: 0.0818649\n",
            "[350]\tvalid_set's binary_logloss: 0.0805544\n",
            "[400]\tvalid_set's binary_logloss: 0.079232\n",
            "[450]\tvalid_set's binary_logloss: 0.0784206\n",
            "[500]\tvalid_set's binary_logloss: 0.0768095\n",
            "[550]\tvalid_set's binary_logloss: 0.0759847\n",
            "[600]\tvalid_set's binary_logloss: 0.075066\n",
            "[650]\tvalid_set's binary_logloss: 0.0743114\n",
            "[700]\tvalid_set's binary_logloss: 0.0736169\n",
            "[750]\tvalid_set's binary_logloss: 0.0727746\n",
            "[800]\tvalid_set's binary_logloss: 0.072003\n",
            "[850]\tvalid_set's binary_logloss: 0.0714918\n",
            "[900]\tvalid_set's binary_logloss: 0.0708359\n",
            "[950]\tvalid_set's binary_logloss: 0.0705871\n",
            "[1000]\tvalid_set's binary_logloss: 0.0700511\n",
            "[1050]\tvalid_set's binary_logloss: 0.0695724\n",
            "[1100]\tvalid_set's binary_logloss: 0.0685792\n",
            "[1150]\tvalid_set's binary_logloss: 0.0681715\n",
            "[1200]\tvalid_set's binary_logloss: 0.0676476\n",
            "[1250]\tvalid_set's binary_logloss: 0.0671719\n",
            "[1300]\tvalid_set's binary_logloss: 0.0667334\n",
            "[1350]\tvalid_set's binary_logloss: 0.0663482\n",
            "[1400]\tvalid_set's binary_logloss: 0.0657725\n",
            "[1450]\tvalid_set's binary_logloss: 0.0653735\n",
            "[1500]\tvalid_set's binary_logloss: 0.0649399\n",
            "[1550]\tvalid_set's binary_logloss: 0.0644775\n",
            "[1600]\tvalid_set's binary_logloss: 0.0640006\n",
            "[1650]\tvalid_set's binary_logloss: 0.0636482\n",
            "[1700]\tvalid_set's binary_logloss: 0.0633845\n",
            "[1750]\tvalid_set's binary_logloss: 0.0629907\n",
            "[1800]\tvalid_set's binary_logloss: 0.0626012\n",
            "[1850]\tvalid_set's binary_logloss: 0.0622521\n",
            "[1900]\tvalid_set's binary_logloss: 0.061879\n",
            "[1950]\tvalid_set's binary_logloss: 0.0614939\n",
            "[2000]\tvalid_set's binary_logloss: 0.0610434\n",
            "[2050]\tvalid_set's binary_logloss: 0.0607815\n",
            "[2100]\tvalid_set's binary_logloss: 0.060575\n",
            "[2150]\tvalid_set's binary_logloss: 0.0602046\n",
            "[2200]\tvalid_set's binary_logloss: 0.0599633\n",
            "[2250]\tvalid_set's binary_logloss: 0.0597704\n",
            "[2300]\tvalid_set's binary_logloss: 0.0596433\n",
            "[2350]\tvalid_set's binary_logloss: 0.0594269\n",
            "[2400]\tvalid_set's binary_logloss: 0.0590983\n",
            "[2450]\tvalid_set's binary_logloss: 0.0587537\n",
            "[2500]\tvalid_set's binary_logloss: 0.0585616\n",
            "[2550]\tvalid_set's binary_logloss: 0.0584289\n",
            "[2600]\tvalid_set's binary_logloss: 0.0583148\n",
            "[2650]\tvalid_set's binary_logloss: 0.0580249\n",
            "[2700]\tvalid_set's binary_logloss: 0.0578604\n",
            "[2750]\tvalid_set's binary_logloss: 0.0575942\n",
            "[2800]\tvalid_set's binary_logloss: 0.0571375\n",
            "[2850]\tvalid_set's binary_logloss: 0.0569676\n",
            "[2900]\tvalid_set's binary_logloss: 0.0568186\n",
            "[2950]\tvalid_set's binary_logloss: 0.0566962\n",
            "[3000]\tvalid_set's binary_logloss: 0.0564787\n",
            "[3050]\tvalid_set's binary_logloss: 0.0563941\n",
            "[3100]\tvalid_set's binary_logloss: 0.0562142\n",
            "[3150]\tvalid_set's binary_logloss: 0.0560174\n",
            "[3200]\tvalid_set's binary_logloss: 0.0559266\n",
            "[3250]\tvalid_set's binary_logloss: 0.0557839\n",
            "[3300]\tvalid_set's binary_logloss: 0.0557038\n",
            "[3350]\tvalid_set's binary_logloss: 0.0554427\n",
            "[3400]\tvalid_set's binary_logloss: 0.0553221\n",
            "[3450]\tvalid_set's binary_logloss: 0.0552577\n",
            "[3500]\tvalid_set's binary_logloss: 0.0551279\n",
            "[3550]\tvalid_set's binary_logloss: 0.0549354\n",
            "[3600]\tvalid_set's binary_logloss: 0.0546532\n",
            "[3650]\tvalid_set's binary_logloss: 0.0545206\n",
            "[3700]\tvalid_set's binary_logloss: 0.054423\n",
            "[3750]\tvalid_set's binary_logloss: 0.0543465\n",
            "[3800]\tvalid_set's binary_logloss: 0.0540874\n",
            "[3850]\tvalid_set's binary_logloss: 0.0540783\n",
            "[3900]\tvalid_set's binary_logloss: 0.0538964\n",
            "[3950]\tvalid_set's binary_logloss: 0.0536463\n",
            "[4000]\tvalid_set's binary_logloss: 0.0535014\n",
            "[4050]\tvalid_set's binary_logloss: 0.053439\n",
            "[4100]\tvalid_set's binary_logloss: 0.0532555\n",
            "[4150]\tvalid_set's binary_logloss: 0.0531245\n",
            "[4200]\tvalid_set's binary_logloss: 0.05311\n",
            "[4250]\tvalid_set's binary_logloss: 0.0530281\n",
            "[4300]\tvalid_set's binary_logloss: 0.0528712\n",
            "[4350]\tvalid_set's binary_logloss: 0.0527055\n",
            "[4400]\tvalid_set's binary_logloss: 0.0524582\n",
            "[4450]\tvalid_set's binary_logloss: 0.0523222\n",
            "[4500]\tvalid_set's binary_logloss: 0.0521923\n",
            "[4550]\tvalid_set's binary_logloss: 0.052057\n",
            "[4600]\tvalid_set's binary_logloss: 0.0519885\n",
            "[4650]\tvalid_set's binary_logloss: 0.0518792\n",
            "[4700]\tvalid_set's binary_logloss: 0.0518194\n",
            "[4750]\tvalid_set's binary_logloss: 0.0517301\n",
            "[4800]\tvalid_set's binary_logloss: 0.0516628\n",
            "[4850]\tvalid_set's binary_logloss: 0.0515811\n",
            "[4900]\tvalid_set's binary_logloss: 0.0515345\n",
            "[4950]\tvalid_set's binary_logloss: 0.0514275\n",
            "[5000]\tvalid_set's binary_logloss: 0.0513377\n",
            "[5050]\tvalid_set's binary_logloss: 0.0512982\n",
            "[5100]\tvalid_set's binary_logloss: 0.0512215\n",
            "[5150]\tvalid_set's binary_logloss: 0.0511228\n",
            "[5200]\tvalid_set's binary_logloss: 0.0510763\n",
            "[5250]\tvalid_set's binary_logloss: 0.0509952\n",
            "[5300]\tvalid_set's binary_logloss: 0.0509429\n",
            "[5350]\tvalid_set's binary_logloss: 0.0508533\n",
            "[5400]\tvalid_set's binary_logloss: 0.0507857\n",
            "[5450]\tvalid_set's binary_logloss: 0.0506821\n",
            "[5500]\tvalid_set's binary_logloss: 0.0506187\n",
            "[5550]\tvalid_set's binary_logloss: 0.0505447\n",
            "[5600]\tvalid_set's binary_logloss: 0.0505195\n",
            "[5650]\tvalid_set's binary_logloss: 0.0505377\n",
            "[5700]\tvalid_set's binary_logloss: 0.0505256\n",
            "[5750]\tvalid_set's binary_logloss: 0.0504427\n",
            "[5800]\tvalid_set's binary_logloss: 0.0503891\n",
            "[5850]\tvalid_set's binary_logloss: 0.0503635\n",
            "[5900]\tvalid_set's binary_logloss: 0.0503202\n",
            "[5950]\tvalid_set's binary_logloss: 0.0503018\n",
            "[6000]\tvalid_set's binary_logloss: 0.0501411\n",
            "[6050]\tvalid_set's binary_logloss: 0.0501291\n",
            "[6100]\tvalid_set's binary_logloss: 0.0500908\n",
            "[6150]\tvalid_set's binary_logloss: 0.0501465\n",
            "[6200]\tvalid_set's binary_logloss: 0.0500573\n",
            "[6250]\tvalid_set's binary_logloss: 0.0500742\n",
            "[6300]\tvalid_set's binary_logloss: 0.0500667\n",
            "[6350]\tvalid_set's binary_logloss: 0.0500999\n",
            "[6400]\tvalid_set's binary_logloss: 0.0500732\n",
            "[6450]\tvalid_set's binary_logloss: 0.0500323\n",
            "[6500]\tvalid_set's binary_logloss: 0.0499541\n",
            "[6550]\tvalid_set's binary_logloss: 0.0498913\n",
            "[6600]\tvalid_set's binary_logloss: 0.0498511\n",
            "[6650]\tvalid_set's binary_logloss: 0.0498793\n",
            "[6700]\tvalid_set's binary_logloss: 0.049809\n",
            "[6750]\tvalid_set's binary_logloss: 0.0497522\n",
            "[6800]\tvalid_set's binary_logloss: 0.0497597\n",
            "[6850]\tvalid_set's binary_logloss: 0.0496923\n",
            "[6900]\tvalid_set's binary_logloss: 0.0496689\n",
            "[6950]\tvalid_set's binary_logloss: 0.0496843\n",
            "[7000]\tvalid_set's binary_logloss: 0.049555\n",
            "[7050]\tvalid_set's binary_logloss: 0.0495069\n",
            "[7100]\tvalid_set's binary_logloss: 0.0493738\n",
            "[7150]\tvalid_set's binary_logloss: 0.0493017\n",
            "[7200]\tvalid_set's binary_logloss: 0.0492907\n",
            "[7250]\tvalid_set's binary_logloss: 0.0492827\n",
            "[7300]\tvalid_set's binary_logloss: 0.0492873\n",
            "[7350]\tvalid_set's binary_logloss: 0.0493031\n",
            "[7400]\tvalid_set's binary_logloss: 0.0491029\n",
            "[7450]\tvalid_set's binary_logloss: 0.0490893\n",
            "[7500]\tvalid_set's binary_logloss: 0.0490899\n",
            "[7550]\tvalid_set's binary_logloss: 0.0491196\n",
            "[7600]\tvalid_set's binary_logloss: 0.049158\n",
            "[7650]\tvalid_set's binary_logloss: 0.0490855\n",
            "[7700]\tvalid_set's binary_logloss: 0.0491246\n",
            "[7750]\tvalid_set's binary_logloss: 0.0490999\n",
            "[7800]\tvalid_set's binary_logloss: 0.0490266\n",
            "[7850]\tvalid_set's binary_logloss: 0.0490529\n",
            "[7900]\tvalid_set's binary_logloss: 0.0490395\n",
            "[7950]\tvalid_set's binary_logloss: 0.0490517\n",
            "[8000]\tvalid_set's binary_logloss: 0.049048\n",
            "[8050]\tvalid_set's binary_logloss: 0.0490572\n",
            "[8100]\tvalid_set's binary_logloss: 0.0490163\n",
            "[8150]\tvalid_set's binary_logloss: 0.0489566\n",
            "[8200]\tvalid_set's binary_logloss: 0.0488763\n",
            "[8250]\tvalid_set's binary_logloss: 0.0487944\n",
            "[8300]\tvalid_set's binary_logloss: 0.0487597\n",
            "[8350]\tvalid_set's binary_logloss: 0.0487596\n",
            "[8400]\tvalid_set's binary_logloss: 0.0487247\n",
            "[8450]\tvalid_set's binary_logloss: 0.0486549\n",
            "[8500]\tvalid_set's binary_logloss: 0.0486637\n",
            "[8550]\tvalid_set's binary_logloss: 0.0487465\n",
            "[8600]\tvalid_set's binary_logloss: 0.048732\n",
            "[8650]\tvalid_set's binary_logloss: 0.0487444\n",
            "[8700]\tvalid_set's binary_logloss: 0.048723\n",
            "[8750]\tvalid_set's binary_logloss: 0.0487071\n",
            "[8800]\tvalid_set's binary_logloss: 0.0487138\n",
            "[8850]\tvalid_set's binary_logloss: 0.0487735\n",
            "[8900]\tvalid_set's binary_logloss: 0.0487383\n",
            "[8950]\tvalid_set's binary_logloss: 0.0486753\n",
            "[9000]\tvalid_set's binary_logloss: 0.0487218\n",
            "[9050]\tvalid_set's binary_logloss: 0.0487299\n",
            "[9100]\tvalid_set's binary_logloss: 0.0487087\n",
            "[9150]\tvalid_set's binary_logloss: 0.0487644\n",
            "[9200]\tvalid_set's binary_logloss: 0.0488556\n",
            "[9250]\tvalid_set's binary_logloss: 0.048981\n",
            "[9300]\tvalid_set's binary_logloss: 0.0490139\n",
            "[9350]\tvalid_set's binary_logloss: 0.0489327\n",
            "[9400]\tvalid_set's binary_logloss: 0.048956\n",
            "[9450]\tvalid_set's binary_logloss: 0.0488385\n",
            "[9500]\tvalid_set's binary_logloss: 0.0488662\n",
            "[9550]\tvalid_set's binary_logloss: 0.0488711\n",
            "[9600]\tvalid_set's binary_logloss: 0.0489029\n",
            "[9650]\tvalid_set's binary_logloss: 0.048924\n",
            "[9700]\tvalid_set's binary_logloss: 0.0490129\n",
            "[9750]\tvalid_set's binary_logloss: 0.0490509\n",
            "[9800]\tvalid_set's binary_logloss: 0.0490408\n",
            "[9850]\tvalid_set's binary_logloss: 0.0489715\n",
            "[9900]\tvalid_set's binary_logloss: 0.0490037\n",
            "[9950]\tvalid_set's binary_logloss: 0.0490062\n",
            "[10000]\tvalid_set's binary_logloss: 0.0490285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving /content/IEEEfraud/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/attr/LightGBMXT/y_pred_proba_val.pkl\n",
            "\t0.9707\t = Validation score   (roc_auc)\n",
            "\t440.06s\t = Training   runtime\n",
            "\t0.99s\t = Validation runtime\n",
            "\t5939.9\t = Inference  throughput (rows/s | 5906 batch size)\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Fitting model: LightGBM ... Training model for up to 429.18s of the 429.17s of remaining time.\n",
            "\tFitting LightGBM with 'num_gpus': 0, 'num_cpus': 6\n",
            "\tFitting with cpus=6, gpus=0, mem=9.7/155.0 GB\n",
            "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50]\tvalid_set's binary_logloss: 0.0958781\n",
            "[100]\tvalid_set's binary_logloss: 0.0878627\n",
            "[150]\tvalid_set's binary_logloss: 0.0833765\n",
            "[200]\tvalid_set's binary_logloss: 0.0809382\n",
            "[250]\tvalid_set's binary_logloss: 0.0788046\n",
            "[300]\tvalid_set's binary_logloss: 0.0760906\n",
            "[350]\tvalid_set's binary_logloss: 0.0747061\n",
            "[400]\tvalid_set's binary_logloss: 0.0730843\n",
            "[450]\tvalid_set's binary_logloss: 0.071836\n",
            "[500]\tvalid_set's binary_logloss: 0.0708809\n",
            "[550]\tvalid_set's binary_logloss: 0.0699904\n",
            "[600]\tvalid_set's binary_logloss: 0.0689386\n",
            "[650]\tvalid_set's binary_logloss: 0.067958\n",
            "[700]\tvalid_set's binary_logloss: 0.0671754\n",
            "[750]\tvalid_set's binary_logloss: 0.0665508\n",
            "[800]\tvalid_set's binary_logloss: 0.0650938\n",
            "[850]\tvalid_set's binary_logloss: 0.0641821\n",
            "[900]\tvalid_set's binary_logloss: 0.0632762\n",
            "[950]\tvalid_set's binary_logloss: 0.0627643\n",
            "[1000]\tvalid_set's binary_logloss: 0.0623129\n",
            "[1050]\tvalid_set's binary_logloss: 0.061703\n",
            "[1100]\tvalid_set's binary_logloss: 0.0612304\n",
            "[1150]\tvalid_set's binary_logloss: 0.0604903\n",
            "[1200]\tvalid_set's binary_logloss: 0.0599091\n",
            "[1250]\tvalid_set's binary_logloss: 0.0594977\n",
            "[1300]\tvalid_set's binary_logloss: 0.0591736\n",
            "[1350]\tvalid_set's binary_logloss: 0.0587748\n",
            "[1400]\tvalid_set's binary_logloss: 0.0584197\n",
            "[1450]\tvalid_set's binary_logloss: 0.0582263\n",
            "[1500]\tvalid_set's binary_logloss: 0.0580032\n",
            "[1550]\tvalid_set's binary_logloss: 0.0574948\n",
            "[1600]\tvalid_set's binary_logloss: 0.0572697\n",
            "[1650]\tvalid_set's binary_logloss: 0.0568967\n",
            "[1700]\tvalid_set's binary_logloss: 0.0565013\n",
            "[1750]\tvalid_set's binary_logloss: 0.0561368\n",
            "[1800]\tvalid_set's binary_logloss: 0.0559027\n",
            "[1850]\tvalid_set's binary_logloss: 0.0556987\n",
            "[1900]\tvalid_set's binary_logloss: 0.0552665\n",
            "[1950]\tvalid_set's binary_logloss: 0.0548023\n",
            "[2000]\tvalid_set's binary_logloss: 0.0543847\n",
            "[2050]\tvalid_set's binary_logloss: 0.0541022\n",
            "[2100]\tvalid_set's binary_logloss: 0.0538497\n",
            "[2150]\tvalid_set's binary_logloss: 0.0535051\n",
            "[2200]\tvalid_set's binary_logloss: 0.053289\n",
            "[2250]\tvalid_set's binary_logloss: 0.0530587\n",
            "[2300]\tvalid_set's binary_logloss: 0.0528047\n",
            "[2350]\tvalid_set's binary_logloss: 0.0525247\n",
            "[2400]\tvalid_set's binary_logloss: 0.0523288\n",
            "[2450]\tvalid_set's binary_logloss: 0.0522128\n",
            "[2500]\tvalid_set's binary_logloss: 0.0520262\n",
            "[2550]\tvalid_set's binary_logloss: 0.0518554\n",
            "[2600]\tvalid_set's binary_logloss: 0.0517785\n",
            "[2650]\tvalid_set's binary_logloss: 0.0516786\n",
            "[2700]\tvalid_set's binary_logloss: 0.0514355\n",
            "[2750]\tvalid_set's binary_logloss: 0.0512051\n",
            "[2800]\tvalid_set's binary_logloss: 0.050966\n",
            "[2850]\tvalid_set's binary_logloss: 0.0508166\n",
            "[2900]\tvalid_set's binary_logloss: 0.0506079\n",
            "[2950]\tvalid_set's binary_logloss: 0.0504496\n",
            "[3000]\tvalid_set's binary_logloss: 0.0503069\n",
            "[3050]\tvalid_set's binary_logloss: 0.0500821\n",
            "[3100]\tvalid_set's binary_logloss: 0.0499221\n",
            "[3150]\tvalid_set's binary_logloss: 0.0496839\n",
            "[3200]\tvalid_set's binary_logloss: 0.0495099\n",
            "[3250]\tvalid_set's binary_logloss: 0.0493596\n",
            "[3300]\tvalid_set's binary_logloss: 0.0493127\n",
            "[3350]\tvalid_set's binary_logloss: 0.0492444\n",
            "[3400]\tvalid_set's binary_logloss: 0.0489531\n",
            "[3450]\tvalid_set's binary_logloss: 0.0488942\n",
            "[3500]\tvalid_set's binary_logloss: 0.0488051\n",
            "[3550]\tvalid_set's binary_logloss: 0.0486601\n",
            "[3600]\tvalid_set's binary_logloss: 0.0485786\n",
            "[3650]\tvalid_set's binary_logloss: 0.048523\n",
            "[3700]\tvalid_set's binary_logloss: 0.0484159\n",
            "[3750]\tvalid_set's binary_logloss: 0.0483385\n",
            "[3800]\tvalid_set's binary_logloss: 0.0481824\n",
            "[3850]\tvalid_set's binary_logloss: 0.0481261\n",
            "[3900]\tvalid_set's binary_logloss: 0.0480608\n",
            "[3950]\tvalid_set's binary_logloss: 0.0479977\n",
            "[4000]\tvalid_set's binary_logloss: 0.0479569\n",
            "[4050]\tvalid_set's binary_logloss: 0.0479582\n",
            "[4100]\tvalid_set's binary_logloss: 0.0478687\n",
            "[4150]\tvalid_set's binary_logloss: 0.0478143\n",
            "[4200]\tvalid_set's binary_logloss: 0.0477406\n",
            "[4250]\tvalid_set's binary_logloss: 0.0477351\n",
            "[4300]\tvalid_set's binary_logloss: 0.0475838\n",
            "[4350]\tvalid_set's binary_logloss: 0.0474459\n",
            "[4400]\tvalid_set's binary_logloss: 0.0472472\n",
            "[4450]\tvalid_set's binary_logloss: 0.0471582\n",
            "[4500]\tvalid_set's binary_logloss: 0.0471204\n",
            "[4550]\tvalid_set's binary_logloss: 0.0470556\n",
            "[4600]\tvalid_set's binary_logloss: 0.0468851\n",
            "[4650]\tvalid_set's binary_logloss: 0.0468152\n",
            "[4700]\tvalid_set's binary_logloss: 0.0467528\n",
            "[4750]\tvalid_set's binary_logloss: 0.0467073\n",
            "[4800]\tvalid_set's binary_logloss: 0.0466769\n",
            "[4850]\tvalid_set's binary_logloss: 0.0466305\n",
            "[4900]\tvalid_set's binary_logloss: 0.0465506\n",
            "[4950]\tvalid_set's binary_logloss: 0.0464969\n",
            "[5000]\tvalid_set's binary_logloss: 0.0463406\n",
            "[5050]\tvalid_set's binary_logloss: 0.0463573\n",
            "[5100]\tvalid_set's binary_logloss: 0.0463709\n",
            "[5150]\tvalid_set's binary_logloss: 0.0463065\n",
            "[5200]\tvalid_set's binary_logloss: 0.0461776\n",
            "[5250]\tvalid_set's binary_logloss: 0.046077\n",
            "[5300]\tvalid_set's binary_logloss: 0.0460447\n",
            "[5350]\tvalid_set's binary_logloss: 0.045993\n",
            "[5400]\tvalid_set's binary_logloss: 0.0459967\n",
            "[5450]\tvalid_set's binary_logloss: 0.0459714\n",
            "[5500]\tvalid_set's binary_logloss: 0.0459115\n",
            "[5550]\tvalid_set's binary_logloss: 0.0459098\n",
            "[5600]\tvalid_set's binary_logloss: 0.0459664\n",
            "[5650]\tvalid_set's binary_logloss: 0.0459954\n",
            "[5700]\tvalid_set's binary_logloss: 0.0459815\n",
            "[5750]\tvalid_set's binary_logloss: 0.046086\n",
            "[5800]\tvalid_set's binary_logloss: 0.0460551\n",
            "[5850]\tvalid_set's binary_logloss: 0.046094\n",
            "[5900]\tvalid_set's binary_logloss: 0.046016\n",
            "[5950]\tvalid_set's binary_logloss: 0.0460385\n",
            "[6000]\tvalid_set's binary_logloss: 0.0460601\n",
            "[6050]\tvalid_set's binary_logloss: 0.0459376\n",
            "[6100]\tvalid_set's binary_logloss: 0.0458995\n",
            "[6150]\tvalid_set's binary_logloss: 0.045897\n",
            "[6200]\tvalid_set's binary_logloss: 0.0459206\n",
            "[6250]\tvalid_set's binary_logloss: 0.0458937\n",
            "[6300]\tvalid_set's binary_logloss: 0.0459139\n",
            "[6350]\tvalid_set's binary_logloss: 0.0458725\n",
            "[6400]\tvalid_set's binary_logloss: 0.0458292\n",
            "[6450]\tvalid_set's binary_logloss: 0.0457981\n",
            "[6500]\tvalid_set's binary_logloss: 0.0457372\n",
            "[6550]\tvalid_set's binary_logloss: 0.0457771\n",
            "[6600]\tvalid_set's binary_logloss: 0.0456735\n",
            "[6650]\tvalid_set's binary_logloss: 0.0456492\n",
            "[6700]\tvalid_set's binary_logloss: 0.0456555\n",
            "[6750]\tvalid_set's binary_logloss: 0.045644\n",
            "[6800]\tvalid_set's binary_logloss: 0.0455322\n",
            "[6850]\tvalid_set's binary_logloss: 0.0455408\n",
            "[6900]\tvalid_set's binary_logloss: 0.0455599\n",
            "[6950]\tvalid_set's binary_logloss: 0.0455074\n",
            "[7000]\tvalid_set's binary_logloss: 0.0455277\n",
            "[7050]\tvalid_set's binary_logloss: 0.04555\n",
            "[7100]\tvalid_set's binary_logloss: 0.0455197\n",
            "[7150]\tvalid_set's binary_logloss: 0.0455584\n",
            "[7200]\tvalid_set's binary_logloss: 0.0455517\n",
            "[7250]\tvalid_set's binary_logloss: 0.0456531\n",
            "[7300]\tvalid_set's binary_logloss: 0.0457002\n",
            "[7350]\tvalid_set's binary_logloss: 0.0456903\n",
            "[7400]\tvalid_set's binary_logloss: 0.0456438\n",
            "[7450]\tvalid_set's binary_logloss: 0.045688\n",
            "[7500]\tvalid_set's binary_logloss: 0.0457036\n",
            "[7550]\tvalid_set's binary_logloss: 0.0457854\n",
            "[7600]\tvalid_set's binary_logloss: 0.045797\n",
            "[7650]\tvalid_set's binary_logloss: 0.0457707\n",
            "[7700]\tvalid_set's binary_logloss: 0.0457807\n",
            "[7750]\tvalid_set's binary_logloss: 0.0457754\n",
            "[7800]\tvalid_set's binary_logloss: 0.0457371\n",
            "[7850]\tvalid_set's binary_logloss: 0.0458464\n",
            "[7900]\tvalid_set's binary_logloss: 0.0458013\n",
            "[7950]\tvalid_set's binary_logloss: 0.045858\n",
            "[8000]\tvalid_set's binary_logloss: 0.0458091\n",
            "[8050]\tvalid_set's binary_logloss: 0.0458456\n",
            "[8100]\tvalid_set's binary_logloss: 0.0459266\n",
            "[8150]\tvalid_set's binary_logloss: 0.0458971\n",
            "[8200]\tvalid_set's binary_logloss: 0.0458865\n",
            "[8250]\tvalid_set's binary_logloss: 0.0458294\n",
            "[8300]\tvalid_set's binary_logloss: 0.0459626\n",
            "[8350]\tvalid_set's binary_logloss: 0.0460894\n",
            "[8400]\tvalid_set's binary_logloss: 0.046082\n",
            "[8450]\tvalid_set's binary_logloss: 0.0460635\n",
            "[8500]\tvalid_set's binary_logloss: 0.0460634\n",
            "[8550]\tvalid_set's binary_logloss: 0.0461094\n",
            "[8600]\tvalid_set's binary_logloss: 0.0461381\n",
            "[8650]\tvalid_set's binary_logloss: 0.0461947\n",
            "[8700]\tvalid_set's binary_logloss: 0.0462171\n",
            "[8750]\tvalid_set's binary_logloss: 0.0461859\n",
            "[8800]\tvalid_set's binary_logloss: 0.0461814\n",
            "[8850]\tvalid_set's binary_logloss: 0.046231\n",
            "[8900]\tvalid_set's binary_logloss: 0.0462975\n",
            "[8950]\tvalid_set's binary_logloss: 0.0463347\n",
            "[9000]\tvalid_set's binary_logloss: 0.046397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving /content/IEEEfraud/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/attr/LightGBM/y_pred_proba_val.pkl\n",
            "\t0.9742\t = Validation score   (roc_auc)\n",
            "\t385.07s\t = Training   runtime\n",
            "\t0.82s\t = Validation runtime\n",
            "\t7217.5\t = Inference  throughput (rows/s | 5906 batch size)\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Fitting model: RandomForestGini ... Training model for up to 43.03s of the 43.02s of remaining time.\n",
            "\tFitting RandomForestGini with 'num_gpus': 0, 'num_cpus': 12\n",
            "\tFitting with cpus=12, gpus=0, mem=0.4/154.7 GB\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/RandomForestGini/model.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/utils/attr/RandomForestGini/y_pred_proba_val.pkl\n",
            "\t0.9319\t = Validation score   (roc_auc)\n",
            "\t238.54s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "\t39753.7\t = Inference  throughput (rows/s | 5906 batch size)\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping RandomForestEntr due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping CatBoost due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping ExtraTreesGini due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping ExtraTreesEntr due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping NeuralNetFastAI due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping XGBoost due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping NeuralNetTorch due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Skipping LightGBMLarge due to lack of time remaining.\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/utils/attr/LightGBM/y_pred_proba_val.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/utils/attr/LightGBMXT/y_pred_proba_val.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/utils/attr/RandomForestGini/y_pred_proba_val.pkl\n",
            "Model configs that will be trained (in order):\n",
            "\tWeightedEnsemble_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -196.23s of remaining time.\n",
            "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 12\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
            "Ensemble size: 17\n",
            "Ensemble weights: \n",
            "[0.17647059 0.82352941 0.        ]\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/utils/oof.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n",
            "\tEnsemble Weights: {'LightGBM': 0.824, 'LightGBMXT': 0.176}\n",
            "\t0.9747\t = Validation score   (roc_auc)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "\t3256.0\t = Inference  throughput (rows/s | 5906 batch size)\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "AutoGluon training complete, total runtime = 1100.42s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3256.0 rows/s (5906 batch size)\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/learner.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/predictor.pkl\n",
            "Saving /content/IEEEfraud/AutoGluonModels/version.txt with contents \"1.4.0\"\n",
            "Saving /content/IEEEfraud/AutoGluonModels/metadata.json\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/IEEEfraud/AutoGluonModels\")\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/RandomForestGini/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.974700     roc_auc       1.813882  825.210078                0.001297           0.080716            2       True          4\n",
            "1             LightGBM   0.974212     roc_auc       0.818289  385.065197                0.818289         385.065197            1       True          2\n",
            "2           LightGBMXT   0.970663     roc_auc       0.994296  440.064165                0.994296         440.064165            1       True          1\n",
            "3     RandomForestGini   0.931882     roc_auc       0.148565  238.538071                0.148565         238.538071            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'RFModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "Plot summary of models saved to file: /content/IEEEfraud/AutoGluonModels/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ],
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
        "    train_data, presets='medium_quality', time_limit=900\n",
        ")\n",
        "\n",
        "results = predictor.fit_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "010f5fef",
      "metadata": {
        "id": "010f5fef"
      },
      "source": [
        "Now, we use the trained AutoGluon Predictor to make predictions on the competition's test data. It is imperative that multiple test data files are joined together in the exact same manner as the training data. Because this competition is evaluated based on the AUC (Area under the ROC curve) metric, we ask AutoGluon for predicted class-probabilities rather than class predictions. In general, when to use `predict` vs `predict_proba` will depend on the particular competition."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_identity = pd.read_csv(directory+'test_identity.csv')\n",
        "test_transaction = pd.read_csv(directory+'test_transaction.csv')\n",
        "\n",
        "# Ensure test_identity has all columns present in train_identity, filling missing ones with NaN\n",
        "train_identity_cols = train_identity.columns\n",
        "test_identity_cols = test_identity.columns\n",
        "missing_cols_in_test_identity = set(train_identity_cols) - set(test_identity_cols)\n",
        "for col in missing_cols_in_test_identity:\n",
        "    test_identity[col] = np.nan\n",
        "\n",
        "test_data = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')  # same join applied to training files\n",
        "\n",
        "y_predproba = predictor.predict_proba(test_data)\n",
        "y_predproba.head(5)  # some example predicted fraud-probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "ZTOP4sl4HqQ7",
        "outputId": "9e9c45d8-88bf-4ae4-a016-c4b373941dd5"
      },
      "id": "ZTOP4sl4HqQ7",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1\n",
              "0  0.999839  0.000161\n",
              "1  0.999992  0.000008\n",
              "2  0.999980  0.000020\n",
              "3  0.999893  0.000107\n",
              "4  0.999973  0.000027"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8d7999b-84ff-4125-a012-647810bab9e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.999839</td>\n",
              "      <td>0.000161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.999992</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.999980</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.999893</td>\n",
              "      <td>0.000107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.999973</td>\n",
              "      <td>0.000027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8d7999b-84ff-4125-a012-647810bab9e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8d7999b-84ff-4125-a012-647810bab9e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8d7999b-84ff-4125-a012-647810bab9e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0683f5e9-1baf-4d0c-9ebd-cb8b84a9a3b7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0683f5e9-1baf-4d0c-9ebd-cb8b84a9a3b7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0683f5e9-1baf-4d0c-9ebd-cb8b84a9a3b7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "y_predproba"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd379db",
      "metadata": {
        "id": "2cd379db"
      },
      "source": [
        "When submitting predicted probabilities for classification competitions, it is imperative these correspond to the same class expected by Kaggle. For binary classification tasks, you can see which class AutoGluon's predicted probabilities correspond to via:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.positive_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNIKJaMIqRS",
        "outputId": "30f0f6d5-7192-4cdb-ca70-c010df637fd3"
      },
      "id": "ADNIKJaMIqRS",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb03739b",
      "metadata": {
        "id": "bb03739b"
      },
      "source": [
        "For multiclass classification tasks, you can see which classes AutoGluon's predicted probabilities correspond to via:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.class_labels  # classes in this list correspond to columns of predict_proba() output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBy2CcQlIt5j",
        "outputId": "0b1133d8-be3f-4325-e794-cab967928926"
      },
      "id": "UBy2CcQlIt5j",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb84c3e",
      "metadata": {
        "id": "8cb84c3e"
      },
      "source": [
        "Now, let's get prediction probabilities for the entire test data, while only getting the positive class predictions by specifying:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predproba = predictor.predict_proba(test_data, as_multiclass=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J78_t_5aIwq6",
        "outputId": "e0f2e493-ef5e-4484-ca69-09078160eaa6"
      },
      "id": "J78_t_5aIwq6",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Loading: /content/IEEEfraud/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f26a317",
      "metadata": {
        "id": "9f26a317"
      },
      "source": [
        "Now that we have made a prediction for each row in the test dataset, we can submit these predictions to Kaggle. Most Kaggle competitions provide a sample submission file, in which you can simply overwrite the sample predictions with your own as we do below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(directory+'sample_submission.csv')\n",
        "submission['isFraud'] = y_predproba\n",
        "submission.head()\n",
        "submission.to_csv(directory+'my_submission.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "GC5ZcQoMJgiZ"
      },
      "id": "GC5ZcQoMJgiZ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2fd99892",
      "metadata": {
        "id": "2fd99892"
      },
      "source": [
        "We have now completed steps (4)-(6) from the top of this tutorial. To submit your predictions to Kaggle, you can run the following command in your terminal (from the appropriate directory):\n",
        "\n",
        "`kaggle competitions submit -c ieee-fraud-detection -f sample_submission.csv -m \"my first submission\"`\n",
        "\n",
        "You can now play with different `fit()` arguments and feature-engineering techniques to try and maximize the rank of your submissions in the Kaggle Leaderboard!\n",
        "\n",
        "\n",
        "**Tips to maximize predictive performance:**\n",
        "\n",
        "   - Be sure to specify the appropriate evaluation metric if one is specified on the competition website! If you are unsure which metric is best, then simply do not specify this argument when invoking `fit()`; AutoGluon should still produce high-quality models by automatically inferring which metric to use.\n",
        "\n",
        "   - If the training examples are time-based and the competition test examples come from future data, we recommend you reserve the most recently-collected training examples as a separate validation dataset passed to `fit()`. Otherwise, you do not need to specify a validation set yourself and AutoGluon will automatically partition the competition training data into its own training/validation sets.\n",
        "\n",
        "   - Beyond simply specifying `presets = 'best_quality'`, you may play with more advanced `fit()` arguments such as: `num_bag_folds`, `num_stack_levels`, `num_bag_sets`, `hyperparameter_tune_kwargs`, `hyperparameters`, `refit_full`. However we recommend spending most of your time on feature-engineering and just specify `presets = 'best_quality'` inside the call to `fit()`.\n",
        "\n",
        "\n",
        "**Troubleshooting:**\n",
        "\n",
        "- Check that you have the right user-permissions on your computer to access the data files downloaded from Kaggle.\n",
        "\n",
        "- For issues downloading Kaggle data or submitting predictions, check your Kaggle account setup and the [Kaggle FAQ](https://www.kaggle.com/general/14438)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}